{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Data Verification Pipeline User Guide"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "* If you opened this notebook from Azure Machine Learning studio, you need a compute instance to run the code. If you don't have a compute instance, select **Create compute** on the toolbar to first create one.  You can use all the default settings.  \n",
    "\n",
    "    ![Create compute](./media/create-compute.png)\n",
    "\n",
    "* If you're seeing this notebook elsewhere, complete [Create resources you need to get started](https://docs.microsoft.com/azure/machine-learning/quickstart-create-resources) to create an Azure Machine Learning workspace and a compute instance.\n",
    "\n",
    "## Set your kernel\n",
    "\n",
    "* If your compute instance is stopped, start it now.  \n",
    "        \n",
    "    ![Start compute](./media/start-compute.png)\n",
    "\n",
    "* Once your compute instance is running, make sure that the kernel, found on the top right, is `Python 3.10 - SDK v2`.  If not, use the dropdown to select this kernel.\n",
    "\n",
    "    ![Set the kernel](./media/set-kernel.png)\n"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the pipeline resources\n",
    "\n",
    "The Azure Machine Learning framework can be used from CLI, Python SDK, or studio interface. In this example, you use the Azure Machine Learning Python SDK v2 to create a pipeline. \n",
    "\n",
    "Before creating the pipeline, you need the following resources:\n",
    "\n",
    "* The data asset for training\n",
    "* The software environment to run the pipeline\n",
    "* A compute resource to run the job\n",
    "\n",
    "## Create handle to workspace\n",
    "\n",
    "Before you deep dive into the code, you need a way to access the workspace. You'll create `ml_client` as a handle to the workspace. Then you can use the `ml_client` to manage resources and jobs.\n",
    "\n",
    "In the next cell, please enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
    "\n",
    "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
    "1. Copy the value for workspace, resource group and subscription ID into the code.\n",
    "1. You'll need to copy one value, close the area and paste, then come back for the next one."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "#\n",
    "# load_dotenv(\"./.env\")\n",
    "# for k in os.environ.keys():\n",
    "#     if \"AZURE\" in k:\n",
    "#         print(\"#####:\",k,os.environ.get(k))\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "# # Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    "    cloud='AzureChinaCloud',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1701335887358
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> [!NOTE]\n",
    "> Creating MLClient will not connect to the workspace. The client initialization is lazy, it will wait for the first time it needs to make a call (this will happen when creating the `credit_data` data asset, two code cells from here).\n",
    "\n",
    "## Register data from an upload file\n",
    "\n",
    "If you have been following along with the other tutorials in this series and already registered the data, you can fetch the same dataset from the workspace using `credit_dataset = ml_client.data.get(\"<DATA ASSET NAME>\", version='<VERSION>')`. Then you may skip this section. To learn about data more in depth or if you would rather complete the data tutorial first, see [Upload, access and explore your data in Azure Machine Learning](https://learn.microsoft.com/azure/machine-learning/tutorial-explore-data).\n",
    "\n",
    "* Azure Machine Learning uses a `Data` object to register a reusable definition of data, and consume data within a pipeline. In the next section, you consume some data from upload file as one example. Data from other sources can be created as well."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "TSR_origin_data = Data(\n",
    "    name=\"TSR_origin_data\",\n",
    "    path='./data/label_train.txt', # path to origin data\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"TSR metadata\",\n",
    "    tags={\"source_type\": \"file\"},\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "\n",
    "Requirments_data = Data(\n",
    "    name=\"TSR_requirements_data\",\n",
    "    path='./data/Data-TSRClassifier.json', # path to requirements_data\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"TSR requirement data\",\n",
    "    tags={\"source_type\": \"file\"},\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "Label_data = Data(\n",
    "    name=\"TSR_label_data\",\n",
    "    path='./data/class.txt', # path to requirements_data\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"TSR label data\",\n",
    "    tags={\"source_type\": \"file\"},\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "# credit_data = ml_client.data.get(name=\"creditcard_defaults\",version=\"1.0.1\")"
   ],
   "outputs": [],
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1701335887508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a compute resource to run your pipeline\n",
    "\n",
    "Each step of an Azure Machine Learning pipeline can use a different compute resource for running the specific job of that step. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
    "\n",
    "In this section, you provision a Linux  [compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). See the [full list on VM sizes and prices](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) .\n",
    "\n",
    "For this tutorial, you only need a basic cluster so use a Standard_DS3_v2 model with 2 vCPU cores, 7-GB RAM and create an Azure Machine Learning Compute.\n",
    "> [!TIP]\n",
    "> If you already have a compute cluster, replace \"cpu-cluster\" in the next code block with the name of your cluster.  This will keep you from creating another one.\n"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure Machine Learning Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "    )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
    }
   ],
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1701335887675
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a job environment for pipeline steps\n",
    "\n",
    "So far, you've created a development environment on the compute instance, your development machine. You also need an environment to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps.\n",
    "\n",
    "In this example, you create a conda environment for your jobs, using a conda yaml file.\n",
    "First, create a directory to store the file in."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1701335887827
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, create the file in the dependencies directory."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - numpy=1.25.0\n",
    "  - pip=23.1.2\n",
    "  - requests==2.25.1\n",
    "  - pip:\n",
    "    - mlflow== 2.4.1\n",
    "    - azureml-mlflow==1.52.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ./dependencies/conda.yml\n"
    }
   ],
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the yaml file to create and register this custom environment in your workspace:"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-data-verification\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for data verification pipeline\",\n",
    "    tags={\"requests\": \"2.25.1\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.0\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Environment with name aml-data-verification is registered to workspace, the environment version is 0.1.0\n"
    }
   ],
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1701335888368
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the pipeline"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
    "\n",
    "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
    "\n",
    "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
    "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
    "- Load that component from the pipeline code.\n",
    "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
    "- Submit the pipeline\n",
    "\n",
    "### Create component 1: data prepare (using programmatic definition)\n",
    "\n",
    "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prepare.py* Python file.\n",
    "\n",
    "First create a source folder for the data_prep component:"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "data_prep_src_dir = \"./components/data_prepare\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1701335888533
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This script converts the original metadata into the format required by the ProSafeAI template. Azure Machine Learning mounts datasets as folders to the computes, therefore, we created an auxiliary `select_first_file` function to access the data file inside the mounted input folder.\n",
    "\n",
    "[MLFlow](https://learn.microsoft.com/articles/machine-learning/concept-mlflow) is used to log the parameters and metrics during our pipeline run."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile {data_prep_src_dir}/data_prepare.py\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "def scan_data():\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--format_data\", type=str, help=\"path to format data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    with open(args.data, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [line.strip().split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", len(data))\n",
    "\n",
    "    for pic_name, value in data:\n",
    "        sub_dict = json.loads(value)\n",
    "\n",
    "        info = {\n",
    "            \"image_name\": pic_name,\n",
    "            \"image_format\": pic_name.split(\".\")[1],\n",
    "            \"class\": sub_dict.get(\"class\"),\n",
    "            \"Snowfall_intensity\": sub_dict.get(\"Snowfall_intensity\"),\n",
    "            \"Fog_intensity\": sub_dict.get(\"Fog_intensity\"),\n",
    "            \"Rain_quantity\": sub_dict.get(\"Rain_quantity\"),\n",
    "            \"dataset\": sub_dict.get(\"dataset\"),\n",
    "            \"augmentation\": sub_dict.get(\"augmentation\"),\n",
    "            \"Illuminance\": sub_dict.get(\"Illuminance\"),\n",
    "        }\n",
    "\n",
    "        results.append(info)\n",
    "\n",
    "    with open(os.path.join(args.format_data, \"TSR_format_metadata.json\"), \"w\", encoding=\"utf-8\") as fw:\n",
    "        json.dump(results, fw)\n",
    "\n",
    "    print('format_data: ', os.path.join(args.format_data, \"TSR_format_metadata.json\"))\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scan_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ./components/data_prepare/data_prepare.py\n"
    }
   ],
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you have a script that can perform the desired task, create an Azure Machine Learning Component from it.\n",
    "\n",
    "Use the general purpose `CommandComponent` that can run command line actions. This command line action can directly call system commands or run a script. The inputs/outputs are specified on the command line via the `${{ ... }}` notation."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prepare_component = command(\n",
    "    name=\"data_prep_metadata\",\n",
    "    display_name=\"Data preparation for metadata\",\n",
    "    description=\"converts the original metadata into the format required by the ProSafeAI template\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        format_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=data_prep_src_dir,\n",
    "    command=\"\"\"python data_prepare.py \\\n",
    "            --data ${{inputs.data}}  --format_data ${{outputs.format_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1701335888820
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally, register the component in the workspace for future reuse."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Now we register the component to the workspace\n",
    "data_prepare_component = ml_client.create_or_update(data_prepare_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prepare_component.name} with Version {data_prepare_component.version} is registered\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Component data_prep_metadata with Version 2023-11-30-09-18-09-0237058 is registered\n"
    }
   ],
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1701335890558
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create component 2: data_import_verification (using yaml definition)\n",
    "\n",
    "The second component that you import the format data into the ProSafeAI backend, and return the data version, then, you run data verification task, and return the report. Use Azure Machine Learning logging capabilities to record and visualize the progress.\n",
    "\n",
    "\n",
    "You used the `CommandComponent` class to create your first component. This time you use the yaml definition to define the second component. Each method has its own advantages. A yaml definition can actually be checked-in along the code, and would provide a readable history tracking. The programmatic method using `CommandComponent` can be easier with built-in class documentation and code completion.\n",
    "\n",
    "Create the directory for this component:"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import_verification_src_dir = \"./components/data_import_verification\"\n",
    "os.makedirs(import_verification_src_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1701335890714
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the import script in the directory:"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile {import_verification_src_dir}/data_import_verification.py\n",
    "\n",
    "import mlflow\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# print(os.getcwd())\n",
    "# /mnt/azureml/cr/j/17e5ad93670542eca36bcdfacb8571ea/exe/wd\n",
    "\n",
    "from prosafeAI_sdk.data_verification import DataVerification\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    print(files[0])\n",
    "\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--format_data\", type=str, help=\"path to format data\")\n",
    "    parser.add_argument(\"--label_data\", type=str, help=\"path to label data\")\n",
    "    parser.add_argument(\"--table_id\", type=int, help=\"table id in ProsafeAI backend\")\n",
    "    parser.add_argument(\"--version_comments\", type=str, help=\"version comments in ProsafeAI backend\")\n",
    "    parser.add_argument(\"--username\", type=str, help=\"username in ProsafeAI backend\")\n",
    "    parser.add_argument(\"--password\", type=str, help=\"password\")\n",
    "    parser.add_argument(\"--requirements_data\", type=str, help=\"path to requirememts json\")\n",
    "    parser.add_argument(\"--task_name\", type=str, help=\"task_name in ProsafeAI backend\")\n",
    "    parser.add_argument(\"--task_report\", type=str, help=\"path to task report\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    data_verification = DataVerification(username=args.username, password=args.password)\n",
    "    data_version = data_verification.import_metadata(\n",
    "            args.table_id, select_first_file(args.format_data), \n",
    "            args.label_data, version_comments=args.version_comments\n",
    "        )\n",
    "\n",
    "    if isinstance(data_version, int):\n",
    "        task_result = data_verification.run_data_verification(\n",
    "            args.table_id,\n",
    "            data_version,\n",
    "            args.requirements_data,\n",
    "            task_name=args.task_name,\n",
    "            task_report=args.task_report\n",
    "        )\n",
    "\n",
    "    # mlflow.log_metric(\"table_id\", args.table_id)\n",
    "    # mlflow.log_metric(\"table_version\", data_version)\n",
    "    # mlflow.log_metric(\"version_comments\", args.version_comments)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ./components/data_import_verification/data_import_verification.py\n"
    }
   ],
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create the yaml file describing the component:"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile {import_verification_src_dir}/data_import_verification.yml\n",
    "# <component>\n",
    "name: data_import_verification_task\n",
    "display_name: Import format metadata into ProsafeAI backend and run data verification task\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  format_data:\n",
    "    type: uri_folder\n",
    "  table_id:\n",
    "    type: number\n",
    "  label_data:\n",
    "    type: uri_folder     \n",
    "  version_comments:\n",
    "    type: string\n",
    "  username:\n",
    "    type: string\n",
    "  password:\n",
    "    type: string\n",
    "  requirements_data:\n",
    "    type: uri_folder\n",
    "  task_name: \n",
    "    type: string\n",
    "outputs:\n",
    "  task_report:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:aml-data-verification:0.1.0\n",
    "command: >-\n",
    "  python data_import_verification.py \n",
    "  --format_data ${{inputs.format_data}} \n",
    "  --label_data ${{inputs.label_data}}\n",
    "  --username ${{inputs.username}} \n",
    "  --password ${{inputs.password}}\n",
    "  --version_comments ${{inputs.version_comments}}\n",
    "  --table_id ${{inputs.table_id}}\n",
    "  --requirements_data ${{inputs.requirements_data}}\n",
    "  --task_name ${{inputs.task_name}}\n",
    "  --task_report ${{outputs.task_report}}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ./components/data_import_verification/data_import_verification.yml\n"
    }
   ],
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now create and register the component. Registering it allows you to re-use it in other pipelines. Also, anyone else with access to your workspace can use the registered component."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "data_import_verification_component = load_component(source=os.path.join(import_verification_src_dir, \"data_import_verification.yml\"))\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_import_verification_component = ml_client.create_or_update(data_import_verification_component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_import_verification_component.name} with Version {data_import_verification_component.version} is registered\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\u001B[32mUploading data_import_verification (0.01 MBs): 100%|██████████| 14500/14500 [00:00<00:00, 83779.19it/s]\n\u001B[39m\n\n"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Component data_import_verification_task with Version 2023-11-30-09-18-11-9654202 is registered\n"
    }
   ],
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1701335893176
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the pipeline from components\n",
    "\n",
    "Now that both your components are defined and registered, you can start implementing the pipeline."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python functions returned by `load_component()` work as any regular Python function that we use within a pipeline to call each step.\n",
    "\n",
    "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\n",
    "\n",
    "Here, we used *input data* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target,\n",
    "    description=\"E2E data_perp-import-verification pipeline\",\n",
    ")\n",
    "def data_verification_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_label_data,\n",
    "    pipeline_job_table_id,\n",
    "    pipeline_job_version_comments,\n",
    "    pipeline_job_username,\n",
    "    pipeline_job_password,\n",
    "    pipeline_job_requirements_json,\n",
    "    pipeline_job_task_name\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prepare_job = data_prepare_component(\n",
    "        data=pipeline_job_data_input,\n",
    "    )\n",
    "\n",
    "    # using data_import_func like a python call with its own inputs\n",
    "    data_import_verification_job = data_import_verification_component(\n",
    "        format_data=data_prepare_job.outputs.format_data,  # note: using outputs from previous step\n",
    "        table_id=pipeline_job_table_id, \n",
    "        label_data=pipeline_job_label_data, \n",
    "        version_comments=pipeline_job_version_comments,  # note: using a pipeline input as parameter\n",
    "        username=pipeline_job_username,\n",
    "        password=pipeline_job_password,\n",
    "        requirements_data=pipeline_job_requirements_json,\n",
    "        task_name=pipeline_job_task_name\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_format_data\": data_prepare_job.outputs.format_data,\n",
    "        # \"pipeline_job_data_version\": data_import_job.outputs.data_version\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1701335893415
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use your pipeline definition to instantiate a pipeline with your dataset."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = data_verification_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=TSR_origin_data.path),\n",
    "    pipeline_job_label_data=Input(type=\"uri_file\", path=Label_data.path),\n",
    "    pipeline_job_table_id=1,\n",
    "    pipeline_job_version_comments=\"MLOps_pipeline\",\n",
    "    pipeline_job_username=\"<USERNAME>\",\n",
    "    pipeline_job_password=\"<PASSWORD>\",\n",
    "    pipeline_job_requirements_json=Input(type=\"uri_file\", path=Requirments_data.path),\n",
    "    pipeline_job_task_name=\"MLOps_pipeline\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1701335893591
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit the job \n",
    "\n",
    "It's now time to submit the job to run in Azure Machine Learning. This time you use `create_or_update`  on `ml_client.jobs`.\n",
    "\n",
    "Here you also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure Machine Learning studio.\n",
    "\n",
    "Once completed, the pipeline registers a model in your workspace as a result of training."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"E2E data_perp-import-verification pipeline\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RunId: frank_animal_kfnx3m6bmf\nWeb View: https://studio.ml.azure.cn/runs/frank_animal_kfnx3m6bmf?wsid=/subscriptions/e0b78dee-edf0-469b-8785-8056794f5e74/resourcegroups/rg-prosafeai-cn/workspaces/rg-prosafeai-cn\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-30 09:18:18Z] Submitting 1 runs, first five are: 64ce4fa4:7cba5140-adb7-4268-bc68-ecc5b1b3c6ff\n[2023-11-30 09:18:23Z] Completing processing run id 7cba5140-adb7-4268-bc68-ecc5b1b3c6ff.\n[2023-11-30 09:18:24Z] Submitting 1 runs, first five are: 430108b7:fefaf68f-a0f4-495a-9308-831738fc5493\n[2023-11-30 09:20:26Z] Completing processing run id fefaf68f-a0f4-495a-9308-831738fc5493.\n\nExecution Summary\n=================\nRunId: frank_animal_kfnx3m6bmf\nWeb View: https://studio.ml.azure.cn/runs/frank_animal_kfnx3m6bmf?wsid=/subscriptions/e0b78dee-edf0-469b-8785-8056794f5e74/resourcegroups/rg-prosafeai-cn/workspaces/rg-prosafeai-cn\n\n"
    }
   ],
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1701336045874
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "name": "python310-sdkv2",
   "language": "python",
   "display_name": "Python 3.10 - SDK v2"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}